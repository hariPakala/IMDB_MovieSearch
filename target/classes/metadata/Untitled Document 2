
% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass[runningheads,a4paper]{llncs}
\usepackage{llncsdoc}
\usepackage{hyperref}
\usepackage{bibentry}
\usepackage{csquotes}
\nobibliography*
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
\graphicspath{{../pdf/}{/home/hari/Downloads/llncs2e/}}
 % and their extensions so you won't have to specify these with
 % every instance of \includegraphics
 \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\begin{document}
\title{Labeling Micro Blog Texts with  Active Learning Methods}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}
%Single insitute
\author{Harish Kumar Pakala \\
\texttt{harish.pakala@st.ovgu.de}
}
\institute{ Faculty of Computer Science,
 Otto-von-Guericke-Universitat Magdeburg.\\
 Universitatsplatz 2, D-39106 Magdeburg, Germany}
%Multiple insitutes
%Currently disabled
%
\iffalse
%Multiple institutes are typeset as follows:
\author{Firstname Lastname\inst{1} \and Firstname Lastname\inst{2} }
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}
\institute{
Insitute 1\\
\email{...}\and
Insitute 2\\
\email{...}
}
\fi
                        
\maketitle
\begin{abstract}
        The advent of social media and in particular micro blogging platforms have enabled a large set of  population to express their views, opinions on the current situations of world, politics, health, sports, stock markets and the list goes on. Apart from being a medium of expression, these platforms are being used to galvanize public opinions on wide range of issues. Processing and analyzing the information being passed over these platforms would enable in taking important decisions as in the case of seeking public pulse for marketing companies, financial institutions, during natural disasters. In such a case creating a model from the existing information and attempting to estimate or predict for future data  would be one scenrio, how ever public opinion need not be consistent, vary or change depending on the circumstances indicating an inconsistent data over time. Active Learning is an incremental approach, where in a model is updated at regular intervals by seeking additional information from a human learner on the new information being analyzed. The key aspect of this approach is to select the most informative data and query to a human learner, here we try to list works of various authors that propose different selection strategies.
        
\end{abstract}
\keywords{Active Learning, Steam Data, Micro Blog}
\section{Introduction}
        Data Streams are sequence of data instances being continuously generated by varied sources or real time operations systems. Micro-blogging services, financial trading floors, geo-spatial services, sensor systems of automated devices like driver less car could be treated as stream data sources or platforms over which the data streams are processed. Processing of such data streams over real time is vital in many application, such as enabling a driver less car to make predictions over real time, analyzing packets over networking systems and reject any injection of malicious packets.
        The micro blogging service twitter is one such stream platform, generates continuous and small text data instances called as tweets. The platform user express their views on diverge topics like US elections, US presidential debates, Floods in India, October Fest in Munich in a 140 characters limit. Usually each topic is associated by a specific tag and these tweets are made accessible to developers so as to make analysis of users behavior over various real world scenarios. Researchers have come up with different application scenarios in analyzing the twitter data, like the works of \cite{streamFinan} where in authors attempt to predict the stock market estimates in the consequent day based on the current day interactions over the micro blog service.
        In case of Stream data, where in applications of the one proposed by \cite{vactive} involve considerable amount of drift and shift over time and has significant effect on the sentiment analysis, as a particular data instance may seem positive at particular instance of time or scenario and the same being negative at other instant. In such scenarios building one initial model and using it over the time to predict any new incoming data instance would  be much less representative to the changing real world scenarios. 
        
        Active learning is one such approach where in, firstly a model is built and from every new batch of data instances that are to labeled few of them are selected and are asked by a human to provide information on them. This new information is considered for updating the model and here the priority or the most important task is to select the informative data instances from a batch of data instances over stream. In this article we attempt to present some of the strategies, that help in retrieving the most informative data instances for labeling from a batch of instances.
        
        The article is organized into 4 sections, in the first the introduction, followed by section 2 detailing about the Active Learning and its need over the continuous data streams, section 3 details about the list of querying strategies, section 4 presents a discussion about all the listed querying strategies and in the section 5 the conclusion.
        
\section{Fundamentals}
\subsection{Micro Blogs}
        Micro Blog is a social media web platform over which users create, exchange small texts, images and video links, where these short texts or messages are also called as micro posts analogous to blogs. Micro blogging has made large number of users to participate in the Internet interactions, the popularity of these platforms has so risen that in the past years they are being substituted to the traditional news media outlets. As pointed in the works of \cite{twitterEx} on June $25{th}$, 2009 only after minutes of Michael Jackson's death huge number of query requests were made to the Google and this incident was one bad day for the organization where in \enquote{ We’re sorry, but your query looks similar to automated requests from a computer virus or spy-ware application. To protect our users, we can’t process your request right now.} result was shown to all the legal users who have suddenly reached out to it to confirm the news. 
        
        On the other hand this information has reached to wider population over micro blogging service twitter and statistics indicate that tweets are exchanged at a rate of 100,000 per hour. The authors of the article \cite{twitterEx} also point another incident in Iran, during the general elections on June $12_th$ 2009 violent protests where broken out on the streets of Tehran and the government had to ban all the social media platforms like Facebook, Youtube and missed in the case of twitter and this facilitated sharing of information about violence to the global audience. As on June 30 2016, with over 1.3 billion registered users, 313 million active users and around 500 million tweets per day indicates that there is a large swathe of information over the platform and seeks for proper attention \cite{aboutTwi}. Researchers from various domain have attempted to extract relevant information form the tweets and used it for making business decisions.
        
        All the micro blogging services along with the twitter are a real time information processing platforms,having the feature of sudden surge or reduction of traffic over different topics or issues. In the area of sentiment classification drift and shift concepts appear where in users preferences on various topic change over time, in such scenarios creation of model with initial set of labeled tweets and using it to label any tweet would not represent the dynamics in the information over time.
\subsection{Active Learning}
        Active learning is a sub domain of machine learning, where in the learning algorithm tries to find the most informative or the important data instances from which it learns and requests a human learner or an oracle for additional information. Figure \ref{figure1} provides an over of the active learning strategy. 
\begin{figure}
\centering 
\includegraphics[width=0.7\textwidth,natwidth=610,natheight=642]{AL_DG}
\caption{Figure depicts process invloved in Active Learning}
\label{figure1}
\end{figure}
        Speech  recognition, image classification, sentiment and opinion mining are few of the applications of active learning. The challenge or the difficulty in this process, is how the learning algorithm selects the most informative data instances to query them to a human, on what basis the informativeness is defined is a point of contention. In the case of support vector machines, the model will be more uncertain in labeling the data instances that are very close to the decision boundary and these instances could be requested for the human learner for manual labeling, this article attempts to present such querying strategies, in the further sections.
\subsection{Strategies involved for fulfillment of this work}
        The main focus our work is to study the importance of micro blogs, the need for Active learning in a stream setting and finally to list and discuss different querying strategies. In the initial stage we are vague in the information search and collection, starting with searching on Micro Blogs and the recent events in the real world and the influence over them through Micro Blogs. 
        
        Instances like 1) Michael Jackson's death information has reached to larger population through twitter in very few minutes, where Google has rejected huge volume of results as auto generated junk queries \cite{twitterEx} 2) On June 12th 2009, all social media platforms are banned in Iran due to election violence, only twitter was missed and the information spread to entire world \cite{twitterEx}. 3) During US Republican presidential debate on Jan 18 2012, Newt Gingrich was asked about his ex-wife, with in minutes negative sentiment increased rapidly \cite{newTG}. 4) During Arab Spring fake and parody account of Egyptian president Hosni Mubarak has tried to garner a lot of political activism against government \cite{aspring}. 
                
        In the next step we have tried to understand Active learning mechanism and its incremental nature to build a model that helps in stream setting where in data instances drastically change over time. In the last we have tried to collect few of the querying strategies from different research works. Figure \ref{figure2} represents a mind map about the sequence steps in the fulfillemnt of this research work.
        
        
\begin{figure}
\centering 
\includegraphics[width=1\textwidth,natwidth=610,natheight=642]{KMD_MIND}
\caption{Figure depicts process invloved in Active Learning}
\label{figure2}
\end{figure}
                
        
\section{Querying Methods in Active Learning}
\subsection{Stream-based active learning for sentiment analysis in the financial domain \cite{streamFinan}}
        Authors of the article have initially considered a collection of 1,600,000 tweets maintained by Stanford University,consisting equal number of positive and
negative tweets. Here tweets are labeled on the assumption of a) Containing ':)' as positive b) Containing ':(' as 'negative' c) Cantoning both are ignored. The tweets are preprocessed by removing all emoticons, replacing @userid's with USER token, web links with URL token, negations (can't, isn't) with token NEGATION and as a bag of words model, a classifier is build using machine algorithm SVM. Here SVM creates a hyper plane that attempts to separate both the classes positive and negative, in another perspective the data points or the tweets near to this hyper plane can be thought of neutral ones and the effort here would be to maximize the distance between this hyper plane and the most the nearest data points, to note the initial collection of tweets are not any domain specific tweets. 
        
        The main goal of the research team is to create financial domain specific predictive model, that would be capable of analyzing the information from a micro blogging platform and be able to predict the outcome of stock markets in the consequent days, for this purpose financial domain tweets are obtained from twitter mircoblog site, as discussed in the section [X.X] we retrieve the information from twitter platform as stream of messages. The stream of tweets are then divided into batches of 100 tweets, and each batch of tweets of classified using the existing model. 
        
        In order to create a more robust model, the authors have proposed an active learning approach, where in from the set of 100 classified tweets, 10 tweets are selected in a such a way that 50 percent of them are closest to the hyper plan, the remaining are chosen randomly and are asked by an expert to label them manually. This new label information is used to rebuild the classifier and the process is repeated for each batch of tweets, on every day, the positive or the negative sentiment probability is calculated by dividing the number of respective tweets by total number. The simulation experiments performed by the authors indicate that “by augmenting a trading strategy with the consideration of changes in positive sentiment probability one could improve the returns”.
\subsection{An adaptive streaming active learning strategy based on instance weighting \cite{badaptive}}
                The authors of article \cite{badaptive} attempt to propose active learning querying strategies, in first a small weight is determined that would be sufficient enough to change the label of a data instance, secondly an adaptive uncertainty threshold which is convenient for evolving streams and that provides compromise between error rate and the number of required labels. 
                
                In the case of a binary classifier say h(x), that classifies a given data instance x to one of the label y1 and y2 and let h(x) classifies x to y1, it is now forced to predict y2 by adding a weight w $\epsilon$ [0,1]. If in such a case smallest w is enough to change the label from y1 to y2 indicates that the model is uncertain in its initial prediction of y1 and here the attempt is to minimize the weight as proposed by authors it can be approximated using dichotomy search. 
                
                In case of a stream based active learning approach a threshold $\theta$ is set, such that for any weight less than $\theta$ the instance is queried for human learning. Here if $\theta$ is very small then extremely few data instances are queried and selected for learning. Contrarily if $\theta$ is high, large number of data instances are queried, the $\theta$ is approximated below a threshold value such that it minimizes the unnecessary queries and prediction errors. In case of unnecessary or large number of data instances selected for labeling $\theta$ value is reduced as per equation \ref{eq1} and in the later case it is increased as per equation \ref{eq2}. 
                
                The authors have conducted experiments on three public datasets “optdigits”, “pendigits” and “segmentation” taken from UCI machine learning repository. The results indicate that adaptive stream based active learner allows the stream-based active learner to achieve an accuracy nearly close to perfect case (fully supervised), with querying much less labels. 
\begin{equation}
\theta_{new} = \theta-\epsilon*(\theta-\hat{w}) 
\label{eq1}
\end{equation}
\begin{equation}
\theta_{new} =  \theta + \epsilon * (\hat{w}*(1-\theta))/\theta
\label{eq2}
\end{equation}
\subsection{Evidence-based uncertainty sampling for active learning \cite{shevidence}}
\label{3.3}
        The authors of the article have proposed an evidence based framework to uncover the reasons for binary classifier's uncertainty in an active learning set up. Usually every data instances are characterized by a set of attributes, here these are grouped into two sets $P_x^{(i)}$ , $N_x^{(i)}$ such that the first one represents the set of attributes that provide evidence for a positive class and the later one represents the set of attributes that provides evidence for a negative class, evidence is defined as the contribution of an attribute of a data instance to the prediction of a particular class. The total evidence that a data instance x belongs to the positive class and evidence belonging to the negative class are given by equations \ref{eq3} and \ref{eq4}. 
        
        Furthermore two terms Conflicting evidence and Insufficient evidence are introduced, conflicting refers to scenario where both the positive, negative evidences are equal and large on the other hand insufficient refers to a situation when both positive and negative evidences are equal and small. Based on these definitions, the authors of the article have come up with three different categories Conflicting-Evidence uncertainty (UNC-CE), Insufficient-Evidence Uncertainty (UNC-IE), Uncertainty Sampling (UNC-t). Among the top 't' uncertain instances UNC-CE picks the instance for which the model is uncertain due to conflicting evidence, UNC-IE picks the instance for which the model is uncertain because of insufficient evidence and UNC-t picks the tth most uncertain instance. 
        
        Along with Random sampling (RND), Uncertainty sampling (UNC-1) and UNC-CE, UNC-IE, UNC-t and the research team has performed tests on eight different publicly available datasets  and making a distinction as conflicting and insufficient evidences provides significant contribution to the performance of a classifier, the insufficient-evidence uncertain instance provides least value in case of active learning and conflicting evidence provides significant contribution to the classifiers efficiency. 
\begin{equation}
E_{+1}(x^{(i)}) = \prod_{x_j^{(i)} \epsilon P_{x^{(i)}}} \frac{P(x_j^{(i)} | +1)}{P(x_j^{(i)} | -1)}
\label{eq3}
\end{equation}
\begin{equation}
E_{-1}(x^{(i)}) = \prod_{x_k^{(i)} \epsilon N_{x^{(i)}}} \frac{P(x_k^{(i)} | +1)}{P(x_k^{(i)} | -1)}
\label{eq4}
\end{equation}
\subsection{Semi-supervised learning combining transductive support vector machine with active learning \cite{tsvmSemi}}
        The authors of the article present a detailed description about Transductive Support Vector Machine (TSVM), which is similar to traditional SVM in finding a hyperplane with largest margin to segregate the data instances and at the same time takes into account of labeled and unlabeled examples, discusses the issues associated with it and attempts to solve by an active learning approach. TSVM is a semi supervised wider-margin classification algorithm on the assumption of low density separation and is formulated as an optimization problem as pointed out in equation \ref{eq5}, here c1 and c2 are user defined parameters responsible for penalizing the misclassified samples (a complete description of TSVM is given in appendix \ref{a.tsvm}). 
        
\begin{equation}
\frac{1}{2} {||W||}^2 + C_1 * \sum_{i=1}^{l}  \xi_i +  C_2 * \sum_{j=i+1}^{l+u}  \xi_j
\label{eq5}
\end{equation}
        
        TSVM provides better performance compared to inductive learning as in this case distribution information is considered, but on the other hand few issues are associated with it in terms of its objective function called the non-convex problem, which is difficult o optimize and also the parameter N needs to be specified in the earlier step of the training process, there is no theoretical proof of higher assumption of N leading to a performance. 
        
        The authors of the article have proposed a semi-supervised learning algorithm combining TSVM and an active learning approach (ALTSVM) to overcome issues with TSVM, here a regularization term is added to the \ref{eq5} that penalizes any abrupt changes in the evaluated function values on the neighbor samples in the laplacian graph  and the resultant objective function is given by the equation \ref{eq6} . 
        
x\begin{equation}
\frac{1}{2} {||W||}^2 + C_1 * \sum_{i=1}^{l}  \xi_i +  C_2 * \sum_{j=i+1}^{l+u}  \xi_j 
+  \sum_{i=i+1}^{l+2u} \beta_i f(x_i)
\label{eq6}
\end{equation}
        Similar to TSVM, for all the unlabeled samples, the objective function values are calculated and the sample x with the minimal objective function is labeled by human, if the label is positive an adjacent instance in the opposite direction of x is selected and labeled, if the the label of x is negative then an instance adjacent to it in the increasing direction of S is labeled by human where this step is done for all samples in each round of interaction or batch requiring labeling. 
        
        Experiements are performed on the data sets "Hepatitis", "WPBC", "Bupa Liver", "Votes" obtained from UCI repository for TSVM, ALTSVM and also for the state of art ofactive learning methods. Results indicate that compared to selection of data samples randomly for querying, ALTSVM has a positive effect on the performance of the classifier as it increases the number of labeled samples.
\subsection{Active Learning with Evolving Streaming Data \cite{vactive}}
        The authors of the article propose two new active learning strategies that handle drift in a stream of data instances. As presented in the section \ref{3.3} uncertainty sampling requires a threshold to be set such that all the data instances below this threshold would be queried for manual labeling, but over an evolving data stream this fixed threshold would result in very high or extremely low number of instances at different instances of time. 
        
        In such scenarios of drift or time variant data streams authors have proposed variable uncertainty threshold, that adjusts itself to he budget depending on the incoming stream. Here if the uncertainty value for a particular data instance is below the threshold value then the threshold is adjusted by a small step value, else if the certainty is good the uncertainty region is made wider or increased by a step value. Another issue with uncertainty sampling is that, it labels the samples that fall with in the threshold region or the ones nearer to the decision boundary, but in the case of evolving drifted data streams changes would happen any where in the instance. 
        
        The authors of the article have proposed another strategy variable randomized uncertainty, here the threshold value is multiplied by a random variable that is normally distributed as N(1,δ), such that the instances that are closed to the decision boundary are labeled more often and also the far away instances are labeled occasionally. 
        
        Experiments are performed on 6 different data sets Electricity, Forest Cover, Airlines, IMDB-E, D and Reuters on the proposed strategies as well Randomized and Fixed Uncertainty, results indicate that proposed strategies perform well when the labeling budget is small and also the Variable uncertainty performs well in cases where drift is not expressed, while in case of strong representation of drift randomized uncertainty performs best.
\section{Evaluation and Discussion}
        In this paper we have focused on Micro blogs and the need for extraction of critical information from them so as to make decisions in real world scenarios. Briefly summarized the need for Active learning in the stream setting. In the evaluation section we would like to present two strategies, firstly to what extent useful information has been extracted from the data instances and secondly to what extent the active learning methods have performed better over the traditional methods as in building an initial classifier and trying to classsify any new data instance on this Opinion model.
        
        The authors of the article \cite{streamFinan} have introduced two tests, one is the Granger causality for correlation between probabilities and stock price for Baidu and the other is F-measure for evaluation of different active learning querying methods and the traditional one. P values of Granger causality indicate that introduction of active learning as improved the overall systems capability in predicting the next day's stock prices of the Baidu Stock. Also among the Active learning approaches, the strategy using 50 percent random and 50 percent closest to the neutral zone.
        
        In the article \cite{shevidence}, the authors did not focused extraction of quality information form the data instances, rather the approach is too evaluate different active learning mehods proposed.
                        
\section{Conclusion}
\section{Appendix} 
\subsection{TSVM} \label{a.tsvm}
The TSVM training process in performed in 4 steps starting with initial assumption of c1 and c2, an initial classifier is modeled by performing an inductive on the labeled samples, here Specify N – an estimated number of positive labeled samples in the unlabeled sampled. In the next step decision function values are calculated for all the unlabeled samples using the initial classifier. The samples having the N largest decision function values are marked as positive and the remaining as negative, now a temporary effect factor ctemp is set. In the third step the model is retrained on all the samples, in the case of new classifier obtained switch one pair of labeled unlabeled samples using a specific rule to minimize the value of the objective function as pointed in equation [X.X]. The switching is process is done until no pair of samples meet the specific switching value. In the step 4 the value Ctemp is uniformly increased   and step 3 is performed, this is done until $C_{temp}$ >= $C_2$.
\subsection{Dichotomy Search} \label{a.ds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs03}
\bibliography{llncs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

